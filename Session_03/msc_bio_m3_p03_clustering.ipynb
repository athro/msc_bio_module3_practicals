{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Example\n",
    "\n",
    "This task lets you explore different clustering approaches (kmeans and hierarchical). First, the data is loaded and you should estimate, how many real cluster where used to generate thes examples. \n",
    "\n",
    "You will use a different approach to measure the quality of the clustering approaches using the teh Silhouette Score. \n",
    "\n",
    "You will compare the clustering to the ground truth answering the question of how good your clustering is, when compared to the real labels. \n",
    "\n",
    "Furthermore, some initail plotting functions are indirectly introduced. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,os.path\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_style(\"white\")\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'mystery_data_a1.csv'\n",
    "df = pd.read_csv(data_file,index_col='id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple plotting\n",
    "\n",
    "Plot the data (as it is only 2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "ax = sns.scatterplot(x=\"x1\", y=\"x2\",data=df,edgecolor='grey',alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means\n",
    "\n",
    "Do a first kmeans clustering using three clusters. Save the generated cluster assignements and score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['x1','x2']]\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, init='random').fit(X)\n",
    "\n",
    "kmeans_centroids            = kmeans.cluster_centers_\n",
    "kmeans_labels_k3            = kmeans.labels_\n",
    "kmeans_labels_cluster_score = kmeans.inertia_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroids\n",
    "\n",
    "Store the centroids and the input data in a new dataframe for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_centroids = pd.DataFrame(kmeans_centroids,columns=['x1','x2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting data and centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "ax = sns.scatterplot(x=\"x1\", y=\"x2\",data=df,edgecolor='grey',alpha=0.5)\n",
    "ax = sns.scatterplot(x=\"x1\", y=\"x2\",data=df_centroids,linewidth=2.0,marker='+',s=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A bit more colourful\n",
    "\n",
    "Same as before, but using the assigned labels for coloring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels_k3'] = kmeans_labels_k3\n",
    "colorPalette='muted'\n",
    "colors = dict(zip(df['labels_k3'].unique(),sns.color_palette(colorPalette)))\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "ax = sns.scatterplot(x=\"x1\", y=\"x2\",hue='labels_k3',palette=colors,data=df,edgecolor='grey',alpha=0.5)\n",
    "ax = sns.scatterplot(x=\"x1\", y=\"x2\",data=df_centroids,linewidth=2.0,marker='+',s=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score\n",
    "\n",
    "The score given by the kmean algorithm is only applicable to kmean and not to other available clustering approaches. An alternative score is the so-called Silhouette Score (see https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient) for more details. It takes into account the mean distance between a sample and all other points in the same class and mean distance between a sample and all other points in the next nearest cluster. The higher this score, the better the underlying clustering approach. The following loads the required parts and applies it to the example before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "metrics.silhouette_score(X, kmeans_labels_k3, metric='euclidean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the 'best' number of clusters\n",
    "\n",
    "The following is a skeleton of a approach of going through k={1..10} to find the best k.\n",
    "\n",
    "```python\n",
    "centroids = {}\n",
    "cluster_score = {}\n",
    "for k in range(1,10):\n",
    "\n",
    "    \n",
    "    df['cluster_k{}'.format(k)] = \n",
    "    cluster_score[k] = kmeans.inertia_ # you might want to use the silhoute score here\n",
    "\n",
    "   \n",
    "```\n",
    "\n",
    "Please fill in the missing parts and plot scores with regrads to k using the following approach:\n",
    "\n",
    "```python\n",
    "df_scores = pd.DataFrame.from_dict(cluster_score,orient='index',columns=['J'])\n",
    "df_scores['k'] = df_scores.index\n",
    "ax = sns.scatterplot(x='k', y='J',data=df_scores)\n",
    "```\n",
    "\n",
    "\n",
    "I.e., you might want to store your results (from each of the k in the loop) in another DataFrame, so it is easier to plot using ```sns.scatterplot(...)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing to Ground Truth \n",
    "\n",
    "The file mystery_data_a1.csv' was generated simple 2D Gaussians. The file 'mystery_data_a1_k.csv' contains the actual labels of each example. Can you load the data and compare the ground thruth (the actual labels given in the additional column) to the ones you have found using your clustering?\n",
    "\n",
    "It might not be totally easy, as the labels generated by the clustering might not be the same by name as the ones given in the file. You might have to do a bit of manual investigation ... \n",
    "\n",
    "However, there exists a method to compare the similarity beteen two clusterings (here: the ground truth and your clustering). The Rand Index does exactly this ( https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rand score example: \n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "adjusted_rand_score([0, 0, 0, 1, 1], [1, 1, 0, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the fully labeled data and compare your clustering. A warning in general: when comparing the labels you usually have to ensure that the data from the ground thruth is for the same example (i.e. the order is the same). Here both files have the same ids in the same order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "## SciPy\n",
    "\n",
    "The first approach is using hierarchical clustering from a different module (SciPy). This is mainly because of its ability to produce a nice dendogram.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you have added addional information to the original dataframe, only take the original data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hier = df[['x1','x2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call to teh clustering is failry simple. Different linckage exsists. Have have a look at: https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n",
    "\n",
    "Try out different settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked = linkage(df_hier[['x1','x2']], 'single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "dendrogram(linked,\n",
    "            orientation='top',\n",
    "            labels=df_hier.index,\n",
    "            show_leaf_counts=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering using sklearn\n",
    "\n",
    "For all options, please have a look at:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_hier\n",
    "hclustering = AgglomerativeClustering(linkage='single',n_clusters=3).fit(X)\n",
    "hclustering_labels_k3  = hclustering.labels_\n",
    "#kmeans_labels_cluster_score = kmeans.inertia_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels_k3'] = hclustering_labels_k3\n",
    "colorPalette='muted'\n",
    "colors = dict(zip(df['labels_k3'].unique(),sns.color_palette(colorPalette)))\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "ax = sns.scatterplot(x=\"x1\", y=\"x2\",hue='labels_k3',palette=colors,data=df,edgecolor='grey',alpha=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best number of clusters using hierarchical clustering\n",
    "\n",
    "Can you re-use your approach from above to estimate the best number of clusters? It should be straight forward, if you have been using the Silhouette Score from above. If you have not done so, please adept this part further up in the notebook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single cell RNA-seq\n",
    "\n",
    "This example will be focused on real world applications of clustering. Consider a single cell RNA-seq dataset, taken from Pollen et al. (2014) study, which consists of 300 single cells (SC), measured across 8686 genes. \n",
    "\n",
    "Potentially reusing some part of your code before, use a clustering approach for different number of clusters k = [1,...,12]. You should establish how many different cell types this dataset might contain In order to determine the most appropriate number of clusters\n",
    "\n",
    "First the data is loaded an processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_libraries_file = 'CellLibraries.txt'\n",
    "df_c = pd.read_csv(cell_libraries_file)\n",
    "\n",
    "pollen_file = 'Pollen2014.txt'\n",
    "df_p = pd.read_csv(pollen_file)\n",
    "\n",
    "df_p = df_p.apply(lambda x : np.log2(x+1)) # log transformation of count data\n",
    "df_p = df_p.transpose() # cells in rows, genes in columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a first glance using the SciPy linkage function, we can look at an initial hierarchical clustering by looking at the dendogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_p = linkage(df_p, 'single')\n",
    "plt.figure(figsize = (10, 10))\n",
    "dendrogram(linked_p,\n",
    "            orientation='top',\n",
    "            labels=df_p.index,\n",
    "            show_leaf_counts=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the data\n",
    "\n",
    "As this is a high-dimanesional dataset, you can use dimensionality reduction methods such as PCA. Other commonly used approaches ate t-SNE or Spectral Embedding. Please note, that these methods can require some time and furthermore might have additional parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(df_p) \n",
    "print(pca.explained_variance_ratio_) # Percentage of variance explained by each of the selected components.\n",
    "df_p_pca = pd.DataFrame(pca.transform(df_p),index=df_p.index,columns=['pca_1','pca_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "\n",
    "ax = sns.scatterplot(x=\"pca_1\", y=\"pca_2\",data=df_p_pca,edgecolor='grey',alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting using Spectral Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import SpectralEmbedding\n",
    "\n",
    "se = SpectralEmbedding(n_components=2)\n",
    "df_p_se = pd.DataFrame(se.fit_transform(df_p),index=df_p.index,columns=['pca_1','pca_2'])\n",
    "plt.figure(figsize = (10, 10))\n",
    "ax = sns.scatterplot(x=\"pca_1\", y=\"pca_2\",data=df_p_se,edgecolor='grey',alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
