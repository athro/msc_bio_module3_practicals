{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - Practical 2\n",
    "\n",
    "After having trained and used your first neural network, you should be able to apply these skills to another data set. \n",
    "\n",
    "The dataset used in this exercise is from the UCI machine learning repository. It consists of measurements of fetal heart rate (FHR) and uterine contraction (UC) features on cardiotocograms classified by expert obstetricians.\n",
    "\n",
    "A more detailed description can be found here: https://archive.ics.uci.edu/ml/datasets/Cardiotocography\n",
    "\n",
    "We have already extracted the main data table and the csv file can be found in the data subdirectory. For reference, the original Excel file is also supplied. \n",
    "\n",
    "The task is to classify the dataset based on the measurements. Here, we use the setting having three classes (according to the NSP column):\n",
    "\n",
    "Normal     = 1\n",
    "Suspect    = 2\n",
    "Pathologic = 3\n",
    "\n",
    "Feel free to use the 10 class version using the CLASS column. \n",
    "\n",
    "Your task in this exercise is to load the data and train a simple neural network to either predict the three classes or later 10 classes. \n",
    "\n",
    "For training purposes it is acceptable to use a train-test split. \n",
    "(However, you might want to evaluate the performance using a 5-fold cross validation. As the presented approach uses the keras module of tensorflow, the GridSearch of sklearn cannot easiliy be applied)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required imports\n",
    "\n",
    "Please note this practical also switched off some warnings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.layers import Input, Dense\n",
    "from tensorflow.python import keras\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data \n",
    "\n",
    "This file contains a not very biological dataset. It is comprised of customers and their shopping behavious. I chose this one, to indicate a bit of pre-processing. A task which will potentially be required by the task for next week. \n",
    "\n",
    "A more detailed introduction in data wrangling will be introduced in another lecture. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/CTG.csv')\n",
    "# drop unused information\n",
    "df = df.drop(['b', 'e', 'Unnamed: 9', 'Unnamed: 31','Unnamed: 42','Unnamed: 44','A','B','C','D','E','AD','DE','LD','FS','SUSP'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the CLASS attribute\n",
    "\n",
    "The column CLASS contains more detailed classification, when compared to NSP. Hence, we do not want to use it for learning and the column is removed. The results is saved in a new dataframe called df_new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.drop(['CLASS'],axis=1)\n",
    "df_new = df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On your own\n",
    "\n",
    "From here on, please use the skills you have learned so far to:\n",
    "\n",
    "1. Split the data into X and y\n",
    "2. Split the result into training and test (or even a 5-fold cross validation)\n",
    "3. Apply scaling for numerical variables and an appropriate encoding for cetegorical ones\n",
    "4. Set up a (multi-)layer neural network\n",
    "5. Train the network and report on its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
