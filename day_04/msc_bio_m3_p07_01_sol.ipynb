{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction Tree Learning\n",
    "\n",
    "In this practical you will have your first real contact supervised machine elarning applied to real biological data. \n",
    "\n",
    "Your task is to establish, which biomarker (or features/attributes) influence the outcome. This execise goes through the clinical biomarkers and has a look at the data using decision trees and random forrests. The author of the paper (see below) has established that no real clinical markers could be found. Instead, he found some other biomarker. The file \n",
    "\n",
    "```\n",
    "'clinical_biomarkers.csv'\n",
    "``` \n",
    "\n",
    "Using initially here, contains the clinical biomarkers and the file\n",
    "\n",
    "```\n",
    "'biomarkers.csv'\n",
    "```\n",
    "\n",
    "the informtive ones. \n",
    "\n",
    "Please go through the exercise/tutorial and establish that you know what you are doing. In a second round use the second file and look into the informative biomarkers. Which one is the most informative on?\n",
    "\n",
    "\n",
    "\n",
    "## Data origin\n",
    "\n",
    "The data originates form the following publication:\n",
    "\n",
    "https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1292-2\n",
    "\n",
    "(going down to section Additional files - Additional file 3 will give you the full ist of raw data)\n",
    "\n",
    "For the purpose of the exercise, we transformed the data already.\n",
    "\n",
    "Before goint into downloading the data - some common imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt # plotting and visulisation\n",
    "import seaborn as sns # nicer (easier) visualisation\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some required import for plotting a learnt tree graphically\n",
    "\n",
    "Please excute this command only if you are sure that you have Graphviz installed. Please also ensure, that you are using the right python versin for pip (in case you have still python2.7 installed)\n",
    "\n",
    "To install graphviz (especially for Windows) have a look here:\n",
    "\n",
    "https://graphviz.gitlab.io/download/\n",
    "\n",
    "You most likely will have to set the Windows PATH variable. Something similar to this one:\n",
    "\n",
    "```!set PATH=%PATH%;C:\\Program Files (x86)\\Graphviz2.38\\bin```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!set PATH=%PATH%;C:\\Program Files (x86)\\Graphviz2.38\\bin # I could not test this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install graphviz --user # or similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assumes that graphviz is instal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# own mini- library\n",
    "import session_helpers\n",
    "import IPython.display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the file and setting the first column to be the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomarkers_file_csv = 'clinical_biomarkers.csv'\n",
    "\n",
    "\n",
    "df = pd.read_csv(biomarkers_file_csv)\n",
    "df = df.set_index(['Sample'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please have a look at the loaded data. How many columns/attributes does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping classes into positive and negative\n",
    "\n",
    "The following maps alle examples either to be positive or begative. Not matching ones ( 'C.'- Control ) are deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex = df.copy()\n",
    "df_ex['Response'] = df_ex['Response'].map(\n",
    "    {\n",
    "     'C.R.':'negative',\n",
    "     'C.':'negative',\n",
    "     'Int. II. R.':'negative',\n",
    "     'High R.':'negative',\n",
    "     'Int. I.':'positive',\n",
    "     'Int. II.':'positive',\n",
    "     'High':'positive',\n",
    "    })\n",
    "\n",
    "\n",
    "df_ex = df_ex.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the values of all columns\n",
    "\n",
    "Here we use the melt function of pandas. This function allows the values to be plotted in a nice fashion. Just click on Run and see. \n",
    "\n",
    "Are you able to spot an attribute or two, separating positive from negative?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_melt = pd.melt(df_ex,id_vars=\"Response\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.boxplot(x=\"features\", y=\"value\", hue=\"Response\", data=plot_data_melt)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Decision Tree Model\n",
    "\n",
    "You might or might not have been able to spot a pattern in the data in order to distinguish positive from negative examples. Here, we build a first decision tree to see what underlying pattern can be found. \n",
    "\n",
    "Before doing this, we split the data into data X and labels y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_ex['Response']\n",
    "X = df_ex.drop(['Response'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "For a initial evaluation of the model, we use a simple train/test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# simple train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import the DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the clasifier\n",
    "\n",
    "In sklean, we first have to set up the decision tree model and then train it using our training data. The model expects at least two inputs: the actual data and the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(random_state=0)\n",
    "dtree = dt_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the learnt tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is a bit dissapointing. You can use the model to predict, but the printout is not very informative. To overcome this, I have written a plotting function (hidden in the session_helpers import from the beginning).\n",
    "\n",
    "### Plotting the  Tree\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to plot the tree inside the model. This will only work when Graphviz and the pyton module for graphviz are installed. \n",
    "\n",
    "You should see something similar to the following:\n",
    "\n",
    "![2 Class Tree](img/tree_2class.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for visulisation:\n",
    "image = session_helpers.plot_tree(dtree,X_test,y_test,rotate=False,max_depth=None)\n",
    "IPython.display.Image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with some of the settings of the decision tree as well as (if you like) with rotate and max_depth in the plotting command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more realistic validation scenario - k-fold cross-validation\n",
    "\n",
    "The learning of the tree in the previous sections was only a first glimpse of a validation. Here we use a cross validation to estimate the performance of the learning algorithm. To do this, we need some additional objects (modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut, GridSearchCV, KFold\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "As we do not want to perform the real splitting away of folds and merging all backtogetehr ourselves, we use the prediefined cross validation function in sklearn. \n",
    "\n",
    "Here, we use a simple 5-fold CV. Have a look what other parameters are possible (this might involve you searching the net!)\n",
    "\n",
    "Within each of the folds, we plot the confusion matrix. Can you change the cose, such that it will calculate the accuracy on each test fold? May be even precision and recall?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=15, shuffle=True)\n",
    "count_k = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    count_k += 1\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test  = X.iloc[test_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test  = y.iloc[test_index]\n",
    "    dtree = dt_model.fit(X_train,y_train)\n",
    "    y_test_predicted = dtree.predict(X_test)\n",
    "    print('Confusion Matrix (k={})'.format(count_k))\n",
    "    print(confusion_matrix(y_test,y_test_predicted))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more realistic setting\n",
    "\n",
    "Actually, the data contained more than two classes. Here we map all 'R.' (Recovery) ones into the class negative and leave the rest as is. \n",
    "\n",
    "Furthermore, we perform the same kind of analysis as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex = df.copy()\n",
    "df_ex['Response'] = df_ex['Response'].map(\n",
    "    {\n",
    "     'C. R.':'negative',\n",
    "     'Int. II. R.':'negative',\n",
    "     'High R.':'negative',\n",
    "     'C.':'C.',\n",
    "     'Int. I.':'Int. I.',\n",
    "     'Int. II.':'Int. II.',\n",
    "     'High':'High',\n",
    "    })\n",
    "df_ex = df_ex.dropna()\n",
    "y = df_ex['Response']\n",
    "X = df_ex.drop(['Response'],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_melt = pd.melt(df_ex,id_vars=\"Response\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(30,10))\n",
    "sns.boxplot(x=\"features\", y=\"value\", hue=\"Response\", data=plot_data_melt)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Train/Test - Decision Tree\n",
    "\n",
    "Warning - more than two classes! What does that mean later on?\n",
    "Just in case Graphviz does not work in your setting. Here is the tree I generated:\n",
    "\n",
    "![5 Class Tree](img/tree_5class.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1)\n",
    "dtree = dt_model.fit(X_train,y_train)\n",
    "# for visulisation:\n",
    "image = session_helpers.plot_tree(dtree,X_test,y_test,rotate=False,max_depth=None)\n",
    "IPython.display.Image(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coss Validation\n",
    "\n",
    "Can you still calculate the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=15, shuffle=True)\n",
    "count_k = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    count_k += 1\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test  = X.iloc[test_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test  = y.iloc[test_index]\n",
    "    dtree = dt_model.fit(X_train,y_train)\n",
    "    y_test_predicted = dtree.predict(X_test)\n",
    "    print('Confusion Matrix (k={})'.format(count_k))\n",
    "    print(confusion_matrix(y_test,y_test_predicted))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "You normal task would be to establish what are the best parameters for each of these folds. Python's sklean offers an easy way to evaluate and test what is the best parameter setting. This way is called grid search. The idea is that you will give a range of hyper-parameters which should be used for testing in the inner loop.  \n",
    "\n",
    "Actually, here we will only do the inner loop on a training and test set setting. Howevewr, you should do this in a real cross validation (outer loop). Furthermore, sklearn can not easily deal with more than two classes in the grid searh and area under curce. Hence, we will be using some form of accuray. Here is a link at possible parameters: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter .\n",
    "\n",
    "To get an idea of what option cann be passed as parameter in the grid search, have a look at the decision tree method of sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "In case you get a warning (red message with DeprecationWarning), please ignore.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'criterion':('gini', 'entropy'), \n",
    "    'max_depth':[1,2,3,4],\n",
    "    'min_samples_leaf':[2,5,10]\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=15)\n",
    "\n",
    "dt_grid_search = GridSearchCV(dt_model, parameters, cv=5,scoring='balanced_accuracy') # weighted == F1 Measure for multi-class\n",
    "grid_search = dt_grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of what the grid search returns as information from the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dt_grid_search.cv_results_.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out what the best score was, we can just save the best performace as number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = max(dt_grid_search.cv_results_['mean_test_score'])\n",
    "best_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. and now look wich parameter setting performed best with that parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter_setting, mean_test_score in zip(dt_grid_search.cv_results_['params'],dt_grid_search.cv_results_['mean_test_score']):\n",
    "    if mean_test_score == best_result:\n",
    "        print('-'*80)\n",
    "        print('BEST RESULTS!!')\n",
    "        print(parameter_setting, mean_test_score)\n",
    "        print('-'*80)\n",
    "    else:\n",
    "        print(parameter_setting, mean_test_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A better way \n",
    "\n",
    "A better way for finding the best performing decision tree, is to directly ask for the best one. Once this is returned, we can use the get_params() method to establish what the set of hyper-parameters were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree_model = dt_grid_search.best_estimator_ # best model according to grid search \n",
    "\n",
    "best_tree_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted = best_tree_model.predict(X_test)\n",
    "\n",
    "print('Confusion Matrix of best model on test')\n",
    "print(confusion_matrix(y_test,y_test_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "\n",
    "If you want to find out, what the most influencial attributes (features or biomarker), we can use the the trees built in information about this. \n",
    "\n",
    "Please note that we use the zip(A,B) method of python to produce a list of tuples from two lists of singletons. I.e. \n",
    "```python \n",
    "zip(['a1','a2','a3'],['b1','b2','b3'])\n",
    "```\n",
    "\n",
    "produces\n",
    "```python \n",
    "[('a1', 'b1'), ('a2', 'b2'), ('a3', 'b3')]\n",
    "```\n",
    "(actually if you want to print is, you will have to put the ```zip()``` into a list : ```list(zip( ... , ... )))```\n",
    "\n",
    "Back to feature importance. Have a look at the most important features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_name,feature_importance in zip(X_test.columns.values,best_tree_model.feature_importances_):\n",
    "    if feature_importance > 0.0:\n",
    "        print('{:20s}:{:3.4f}'.format(feature_name,feature_importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "Let us repeat this exercise with Random Forrests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'n_estimators': [2,3,5], \n",
    "    'max_depth':[1,2,3,4],\n",
    "    'min_samples_leaf':[2,5,10]\n",
    "}\n",
    "\n",
    "random_f_model = RandomForestClassifier() \n",
    "rf_grid_search = GridSearchCV(random_f_model, parameters, cv=5,scoring='balanced_accuracy') # weighted == F1 Measure for multi-class\n",
    "grid_search = rf_grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_f_model = rf_grid_search.best_estimator_ # best model according to grid search \n",
    "\n",
    "best_random_f_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted = best_random_f_model.predict(X_test)\n",
    "\n",
    "print('Confusion Matrix of best model on test')\n",
    "print(confusion_matrix(y_test,y_test_predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most important biomarkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_name,feature_importance in zip(X_test.columns.values,best_random_f_model.feature_importances_):\n",
    "    if feature_importance > 0.0:\n",
    "        print('{:20s}:{:3.4f}'.format(feature_name,feature_importance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the clinical biomarkers\n",
    "\n",
    "Just exchange the two filenames:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#biomarkers_file_csv = 'clinical_biomarkers.csv'\n",
    "biomarkers_file_csv = 'biomarkers.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can re-run the complete exersice or just concentrate on the essentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bio = pd.read_csv(biomarkers_file_csv)\n",
    "df_bio = df_bio.set_index(['Sample'])\n",
    "\n",
    "df_bio_ex = df_bio.copy()\n",
    "\n",
    "df_bio_ex['Response'] = df_bio_ex['Response'].map(\n",
    "    {\n",
    "     'C. R.':'negative',\n",
    "     'Int. II. R.':'negative',\n",
    "     'High R.':'negative',\n",
    "     'C.':'C.',\n",
    "     'Int. I.':'Int. I.',\n",
    "     'Int. II.':'Int. II.',\n",
    "     'High':'High',\n",
    "    })\n",
    "df_bio_ex = df_bio_ex.dropna()\n",
    "\n",
    "y = df_bio_ex['Response']\n",
    "X = df_bio_ex.drop(['Response'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_melt_bio = pd.melt(df_bio_ex,id_vars=\"Response\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(30,10))\n",
    "sns.boxplot(x=\"features\", y=\"value\", hue=\"Response\", data=plot_data_melt_bio)\n",
    "plt.xticks(rotation=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'n_estimators': [2,3,5], \n",
    "    'max_depth':[1,2,3,4],\n",
    "    'min_samples_leaf':[2,5,10]\n",
    "}\n",
    "\n",
    "\n",
    "random_f_model = RandomForestClassifier() \n",
    "\n",
    "\n",
    "\n",
    "kf1 = KFold(n_splits=5, random_state=15, shuffle=True)\n",
    "\n",
    "count_k = 0\n",
    "for train_index, test_index in kf.split(X):    \n",
    "    count_k += 1\n",
    "    # set up train and test  data\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test  = X.iloc[test_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test  = y.iloc[test_index]\n",
    "    \n",
    "    \n",
    "    # set up grid search\n",
    "    rf_grid_search = GridSearchCV(random_f_model, parameters, cv=5,scoring='balanced_accuracy') \n",
    "    grid_search_result = rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # get best model\n",
    "    best_rf_model = rf_grid_search.best_estimator_ # best model according to grid search \n",
    "    \n",
    "    # print 'best' mdoels paramter\n",
    "    best_rf_model_parameters = best_rf_model.get_params()\n",
    "    print('k = {}'.format(count_k))\n",
    "    for parameter in parameters:\n",
    "        print('{:30}\\t{}'.format(parameter,best_rf_model_parameters[parameter]))\n",
    "    print()\n",
    "\n",
    "    # print confusion matrix\n",
    "    y_test_predicted = best_rf_model.predict(X_test)\n",
    "\n",
    "    print('Confusion Matrix on test')\n",
    "    print(confusion_matrix(y_test,y_test_predicted))     \n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # get feature importance\n",
    "    feature_importances = list(zip(best_rf_model.feature_importances_,X_test.columns.values))\n",
    "    feature_importances.sort(reverse=True)\n",
    "    \n",
    "    # only plot the top 5 (please adopt)\n",
    "    for feature_importance,feature_name in feature_importances[:5]:\n",
    "        print('{:20s}:{:3.4f}'.format(feature_name,feature_importance))\n",
    "\n",
    "\n",
    "    print('-'*60)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
