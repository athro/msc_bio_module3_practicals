{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - Practical 1\n",
    "\n",
    "In this an initial practical for neural neyworks. It does load an existing dataset of 10k customers, pre-processing it, and learns an initial neural network model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required imports\n",
    "\n",
    "Please note this practical also switched of some warnings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.layers import Input, Dense\n",
    "from tensorflow.python import keras\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data \n",
    "\n",
    "This file contains a not very biological dataset. It is comprised of customers and their shopping behavious. I chose this one, to indicate a bit of pre-processing. A task which will potentially be required by the task for next week. \n",
    "\n",
    "A more detailed introduction in data wrangling will be introduced in another lecture. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_file = './data/Churn_Modelling.csv'\n",
    "\n",
    "df_churn = pd.read_csv(churn_file)\n",
    "#df_churn_attributes = list(df_churn.columns.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop non-data columns\n",
    "\n",
    "Some of the columns contain very specific information. Here, we are not using these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a copied dataframe without non-data columns\n",
    "df_churn_copy = df_churn.drop(['RowNumber', 'Surname', 'CustomerId'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a look at the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churn_copy.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding string data\n",
    "\n",
    "To encode categorical data such as in the columns Geography and Gender, we use the LabelEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder_geography = LabelEncoder()\n",
    "df_churn_copy['Geography'] = labelencoder_geography.fit_transform(df_churn_copy['Geography'])\n",
    "\n",
    "labelencoder_gender = LabelEncoder()\n",
    "df_churn_copy['Gender'] = labelencoder_gender.fit_transform(df_churn_copy['Gender'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churn['Geography'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical data using one-hot-encoding\n",
    "\n",
    "Not all modelling algorithms can easily cope with categorical data. Within this section we will map categorical to  numerical ones. \n",
    "\n",
    "As an example, one could map \n",
    "\n",
    "| Geography | Geography_mapped |\n",
    "| ------------- |:-------------:|\n",
    "| France       | 0 |\n",
    "| Spain        | 1 |\n",
    "| Germany      | 2 |\n",
    "\n",
    "As we have done above (see sectin just before).\n",
    "\n",
    "However, one problem with this approach is that numerical values have a inherent ordinal meaning. e.g. if one would like to know how similar ```'France'``` to ```'Spain'``` or ```'France'``` to ```'Germany'``` one would after mapping compare 0 to 1 or 0 to 2. For algorithms, and especially Neural Networks these are different meanings. \n",
    "\n",
    "One approach is to encode this into the so called one-hot encoding. Here, one would create additional variables for each of the possible values. The mapping could look like the following:\n",
    "\n",
    "| Geography | Geography__France | Geography__Spain | Geography__Germany | \n",
    "| ------------- |:-------------:|:-------------:|:-------------:|\n",
    "| France      | 1 | 0 | 0 | \n",
    "| Spain       | 0 | 1 | 0 | \n",
    "| Germany     | 0 | 0 | 1 | \n",
    "\n",
    "For simplicity in the later session, we will use both approaches. But, please keep in mind that some of the results should be taken with a pinch of salt, when using the simply mapped version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only one-hot encode column index number 1 (i.e. the second one)\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "df_churn_copy2 = pd.DataFrame(onehotencoder.fit_transform(df_churn_copy).toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "For simplicity of the exercise, we just use a train-test split. Please feel free to do a propper CV in your own time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df_churn_copy2[range(12)]\n",
    "y = df_churn_copy2[12]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "The datasets in other practicals were already scaled. Here, we do the scaling. Remeber, that one should apply pre-processing egnerally only on the training data. Hence, here we use the StandardScaler and fit the scaling on the training data only. The learnt scaling is then applied to the test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning your first Neural Network\n",
    "\n",
    "We use the Keras module of Tensorflow. This allows to combine NNs using a sequence of layers (using Sequential). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing Neural Network\n",
    "neural_network = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the layers\n",
    "\n",
    "We subsequently add two layers (the input layer needs only indirectly be described in the first layer by defining the input dimension). \n",
    "\n",
    "The first layer takes in a 12 dimensional vector, uses ReLu as activation function and has 6 hidden nodes. \n",
    "\n",
    "The output layer takes the 6 outputs from the hidden layer, uses the Sigmoid activation function and returns a single output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the first hidden layer\n",
    "\n",
    "neural_network.add(Dense(activation = 'relu', input_dim = 12, units=6))\n",
    "neural_network.add(Dense(activation = 'sigmoid', units=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the network\n",
    "\n",
    "The network needs to be compiled for tensorflow. Here we are using ADAM as optimiser (in contrast to simple gradient descent and learning rate) and measure the performance on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling Neural Network\n",
    "neural_network.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "Here, we train the network. \n",
    "\n",
    "\n",
    "'batch_size' defines in what size of batches the examples are presented and the gradient is calculated\n",
    "\n",
    "'epochs' defines how many epochs is used for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting our model \n",
    "neural_network.fit(X_train, y_train, batch_size = 10, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = neural_network.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional layers\n",
    "\n",
    "One can also add additional layers to design a 'deeper' network. \n",
    "\n",
    "Please observe how the metric (accuracy changes over each epoch).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing Neural Network\n",
    "neural_network = Sequential()\n",
    "neural_network.add(Dense(activation = 'relu', input_dim = 12, units=6))\n",
    "neural_network.add(Dense(activation = 'relu', units=6))\n",
    "neural_network.add(Dense(activation = 'relu', units=6))\n",
    "neural_network.add(Dense(activation = 'relu', units=6))\n",
    "neural_network.add(Dense(activation = 'relu', units=6))\n",
    "neural_network.add(Dense(activation = 'sigmoid', units=1))\n",
    "neural_network.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "neural_network.fit(X_train, y_train, batch_size = 10, nb_epoch = 10)\n",
    "y_pred = neural_network.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid as the only activation function \n",
    "\n",
    "What happens, when we use the same design, but just use the Sigmoid function?\n",
    "\n",
    "Why does this happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing Neural Network\n",
    "neural_network = Sequential()\n",
    "neural_network.add(Dense(activation = 'sigmoid', input_dim = 12, units=6))\n",
    "neural_network.add(Dense(activation = 'sigmoid', units=6))\n",
    "neural_network.add(Dense(activation = 'sigmoid', units=6))\n",
    "neural_network.add(Dense(activation = 'sigmoid', units=6))\n",
    "neural_network.add(Dense(activation = 'sigmoid', units=6))\n",
    "neural_network.add(Dense(activation = 'sigmoid', units=1))\n",
    "neural_network.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "neural_network.fit(X_train, y_train, batch_size = 10, nb_epoch = 10)\n",
    "y_pred = neural_network.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple classes\n",
    "\n",
    "We currently wored on binary classes. To enable the NN to work on multiple classes, we have to add some additional archictecture around. \n",
    "\n",
    "For this example, we first convert the binary class into a binary vector of length 2: one for each class\n",
    "We use this as output for training. To normalize the output for all possible classes (here just two) we use the softmax activation mapping the original output into probabilities. The effect is that the prediction will now be teh probability for each class. \n",
    "\n",
    "Have a look at the following code.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder_labels = OneHotEncoder()\n",
    "\n",
    "onehotencoder_labels.fit(np.array([y_train]).transpose()) \n",
    "\n",
    "# ecode using the new representation\n",
    "y2_train = onehotencoder_labels.transform(np.array(np.array([y_train]).transpose())).toarray()\n",
    "y2_test = onehotencoder_labels.transform(np.array(np.array([y_test]).transpose())).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing Neural Network\n",
    "neural_network = Sequential()\n",
    "neural_network.add(Dense(activation = 'relu', input_dim = 12, units=6))\n",
    "neural_network.add(Dense(activation = 'sigmoid', units=6))\n",
    "neural_network.add(Dense(activation = 'softmax', units=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "neural_network.fit(X_train, y2_train, batch_size = 10, nb_epoch = 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = neural_network.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
