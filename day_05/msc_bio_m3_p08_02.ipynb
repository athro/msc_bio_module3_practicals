{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVMs Practical 2\n",
    "In this exercise, you will learn how to select the hyperparameter in SVM.\n",
    "You will need a library called mlxtrend. You can install using: pip3 install --user mlxtend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install mlxtend --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import neccessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn import svm # \"Support vector classifier\"\n",
    "from sklearn import datasets\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(X, Y, model, xlim = None, ylim = None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    \n",
    "    if (xlim and ylim) is None:\n",
    "        xlim = [min(X[:,0]), max(X[:, 0])]\n",
    "        ylim = [min(X[:,1]), max(X[:, 1])]\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    yy, xx = np.meshgrid(y, x)\n",
    "    xy = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "    \n",
    "    #if more than 2 classes, just plot boundary, otherwise plot margin as well\n",
    "    if len(model.classes_) > 2:\n",
    "        Z = model.predict(xy)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.2)\n",
    "    else:\n",
    "        P = model.decision_function(xy).reshape(xx.shape)\n",
    "        # plot decision boundary and margins\n",
    "        plt.contour(xx, yy, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        plt.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none', edgecolors='black');\n",
    "        \n",
    "    for yy in set(Y):\n",
    "        plt.scatter(X[Y == yy, 0], X[Y == yy, 1], label = \"Class \" + str(yy))\n",
    "    \n",
    "    plt.legend(loc = 'best')\n",
    "    \n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's see the effect of gamma and C in RBF kernel SVM.\n",
    "\n",
    "# Generate an artificial data set, with 500 data points and 2 classes.\n",
    "\n",
    "X, y = make_blobs(n_samples=200, centers=2, random_state=2, cluster_std=0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create RBF SVM model as what we have done in exercise 1, however, use gamme= [10^-2, 10^0, 10^2]\n",
    "gammas = [-2,0,2]\n",
    "counter = 0\n",
    "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "for gamma in gammas:\n",
    "    counter = counter + 1\n",
    "    model = svm.SVC(kernel = 'rbf', gamma=pow(10,gamma))\n",
    "    model.fit(X, y)\n",
    "    plt.subplot(1, len(gammas),counter)\n",
    "    plot_svc_decision_function(X, y, model)\n",
    "    plt.title(\"Gamma: \" + str(pow(10,gamma)))\n",
    "plt.rcParams.update(plt.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [-2,0,2]\n",
    "counter = 0\n",
    "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "for c in C:\n",
    "    counter = counter + 1\n",
    "    model = svm.SVC(kernel = 'rbf', gamma = 'auto', C = pow(2, c))\n",
    "    model.fit(X, y)\n",
    "    plt.subplot(1, len(C),counter)\n",
    "    plot_svc_decision_function(X, y, model)\n",
    "    plt.title(\"C: \" + str(pow(10,c)))\n",
    "plt.rcParams.update(plt.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try on another data set.\n",
    "X, y = datasets.make_moons(n_samples=100, random_state=123)\n",
    "\n",
    "#Create RBF SVM model as what we have done in exercise 1, however, use gamme= [10^-2, 10^0, 10^2]\n",
    "gammas = [-2,0,2]\n",
    "counter = 0\n",
    "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "for gamma in gammas:\n",
    "    counter = counter + 1\n",
    "    model = svm.SVC(kernel = 'rbf', gamma=pow(10,gamma))\n",
    "    model.fit(X, y)\n",
    "    plt.subplot(1, len(gammas),counter)\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=model, legend=2)\n",
    "    plt.title(\"Gamma: \" + str(pow(10,gamma)))\n",
    "plt.rcParams.update(plt.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [-2,0,2]\n",
    "counter = 0\n",
    "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "for c in C:\n",
    "    counter = counter + 1\n",
    "    model = svm.SVC(kernel = 'rbf', gamma = 'auto', C = pow(2, c))\n",
    "    model.fit(X, y)\n",
    "    plt.subplot(1, len(C),counter)\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=model, legend=2)\n",
    "    plt.title(\"C: \" + str(pow(10,c)))\n",
    "plt.rcParams.update(plt.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: \n",
    "\n",
    "What do you think is the effect of different gamma, and C in the data set? How does changing gamma and/or C affect the decision boundary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: \n",
    "\n",
    "Try changing the code to play with different gamma and C values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use a grid search to find the optimum values of gamma and C. \n",
    "\n",
    "Note that, in all previous examples, all data points were used to develop the model, however, in practice, we need to split the data into three main chucks i.e. training, validation, and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we split the data into train and testing set using hold-out technique. \n",
    "# We use 80% of the data to train and select the hyper parameters.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1, 1e-2, 1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "# Specify the evaluation metric we will use to select our hyper parameters. In this example, we will use auc.\n",
    "scores = ['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(svm.SVC(), tuned_parameters, cv=5,\n",
    "                       scoring=score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: \n",
    "\n",
    "Plot the decision boundary using the gamma and C identified from grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
